{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import math\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "from sklearn.model_selection import KFold\n",
    "# Setting seed for reproducibiltiy\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_path(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        images.append(tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(os.path.join(path, file), target_size=(32, 32, 3))))\n",
    "        labels.append((label))\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_train = []\n",
    "mel_test = []\n",
    "label_train = []\n",
    "label_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images_from_path('', 0)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('', 1)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('', 2)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('', 3)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('', 4)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images_from_path('', 0)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('', 1)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('t', 2)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('', 3)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('', 4)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mel_train_norm = np.array(mel_train) / 255\n",
    "mel_test_norm = np.array(mel_test) / 255\n",
    "\n",
    "\n",
    "label_train_encoded = to_categorical(label_train)\n",
    "label_test_encoded = to_categorical(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "BUFFER_SIZE = \n",
    "BATCH_SIZE = \n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = \n",
    "PATCH_SIZE = \n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = \n",
    "WEIGHT_DECAY = \n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = \n",
    "\n",
    "# ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "TRANSFORMER_LAYERS = 8\n",
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "MLP_HEAD_UNITS = [2048, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(mel_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedPatchTokenization(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        projection_dim=PROJECTION_DIM,\n",
    "        vanilla=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.half_patch = patch_size // 2\n",
    "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "\n",
    "    def crop_shift_pad(self, images, mode):\n",
    "        # Build the diagonally shifted images\n",
    "        if mode == \"left-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = 0\n",
    "            shift_width = 0\n",
    "        elif mode == \"left-down\":\n",
    "            crop_height = 0\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = 0\n",
    "        elif mode == \"right-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = 0\n",
    "            shift_height = 0\n",
    "            shift_width = self.half_patch\n",
    "        else:\n",
    "            crop_height = 0\n",
    "            crop_width = 0\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = self.half_patch\n",
    "\n",
    "        # Crop the shifted images and pad them\n",
    "        crop = tf.image.crop_to_bounding_box(\n",
    "            images,\n",
    "            offset_height=crop_height,\n",
    "            offset_width=crop_width,\n",
    "            target_height=self.image_size - self.half_patch,\n",
    "            target_width=self.image_size - self.half_patch,\n",
    "        )\n",
    "        shift_pad = tf.image.pad_to_bounding_box(\n",
    "            crop,\n",
    "            offset_height=shift_height,\n",
    "            offset_width=shift_width,\n",
    "            target_height=self.image_size,\n",
    "            target_width=self.image_size,\n",
    "        )\n",
    "        return shift_pad\n",
    "\n",
    "    def call(self, images):\n",
    "        if not self.vanilla:\n",
    "            # Concat the shifted images with the original image\n",
    "            images = tf.concat(\n",
    "                [\n",
    "                    images,\n",
    "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "        # Patchify the images and flatten it\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        flat_patches = self.flatten_patches(patches)\n",
    "        if not self.vanilla:\n",
    "            # Layer normalize the flat patches and linearly project it\n",
    "            tokens = self.layer_norm(flat_patches)\n",
    "            tokens = self.projection(tokens)\n",
    "        else:\n",
    "            # Linearly project the flat patches\n",
    "            tokens = self.projection(flat_patches)\n",
    "        return (tokens, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image from the training dataset\n",
    "# and resize the image\n",
    "image = mel_train_norm[np.random.choice(range(mel_train_norm.shape[0]))]\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    ")\n",
    "\n",
    "# Vanilla patch maker: This takes an image and divides into\n",
    "# patches as in the original ViT paper\n",
    "(token, patch) = ShiftedPatchTokenization(vanilla=True)(resized_image / 255.0)\n",
    "(token, patch) = (token[0], patch[0])\n",
    "n = patch.shape[0]\n",
    "count = 1\n",
    "plt.figure(figsize=(4, 4))\n",
    "for row in range(n):\n",
    "    for col in range(n):\n",
    "        plt.subplot(n, n, count)\n",
    "        count = count + 1\n",
    "        image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 3))\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Shifted Patch Tokenization: This layer takes the image, shifts it\n",
    "# diagonally and then extracts patches from the concatinated images\n",
    "(token, patch) = ShiftedPatchTokenization(vanilla=False)(resized_image / 255.0)\n",
    "(token, patch) = (token[0], patch[0])\n",
    "n = patch.shape[0]\n",
    "shifted_images = [\"ORIGINAL\", \"LEFT-UP\", \"LEFT-DOWN\", \"RIGHT-UP\", \"RIGHT-DOWN\"]\n",
    "for index, name in enumerate(shifted_images):\n",
    "    print(name)\n",
    "    count = 1\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for row in range(n):\n",
    "        for col in range(n):\n",
    "            plt.subplot(n, n, count)\n",
    "            count = count + 1\n",
    "            image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 5 * 3))\n",
    "            plt.imshow(image[..., 3 * index : 3 * index + 3])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "\n",
    "    def call(self, encoded_patches):\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_patches = encoded_patches + encoded_positions\n",
    "        return encoded_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is\n",
    "        # the square root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Build the diagonal attention mask\n",
    "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier(vanilla=False):\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder()(tokens)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        if not vanilla:\n",
    "            attention_output = MultiHeadAttentionLSA(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1, attention_mask=diag_attn_mask)\n",
    "        else:\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(5)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a checkpoint callback to save the best model during training\n",
    "checkpoint_path = \"\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",  # Save the model with the lowest validation loss\n",
    "    mode=\"min\",           # \"min\" means lower validation loss is better\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    total_steps = int((len(mel_train_norm) / BATCH_SIZE) * EPOCHS)\n",
    "    warmup_epoch_percentage = 0.10\n",
    "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "    scheduled_lrs = WarmUpCosine(\n",
    "        learning_rate_base=LEARNING_RATE,\n",
    "        total_steps=total_steps,\n",
    "        warmup_learning_rate=0.0,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            CategoricalAccuracy(name=\"accuracy\"),\n",
    "            TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Add the model checkpoint callback to the list of callbacks\n",
    "    callbacks = [model_checkpoint_callback]\n",
    "\n",
    "    history = model.fit(\n",
    "        x=mel_train_norm,\n",
    "        y=label_train_encoded,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.1,\n",
    "        callbacks=callbacks  # Pass the callbacks to the fit method\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(mel_test_norm, label_test_encoded, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "num_folds = \n",
    "\n",
    "# Initialize a KFold object\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results from each fold\n",
    "all_train_histories = []\n",
    "all_test_accuracies = []\n",
    "all_test_top_5_accuracies = []\n",
    "\n",
    "# Define whether to run vanilla ViT or modified ViT\n",
    "run_vanilla_vit = True  # Set to False to run modified ViT\n",
    "\n",
    "# Loop over the folds\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(mel_train_norm)):\n",
    "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "    \n",
    "    # Split the data into train and test sets for this fold\n",
    "    x_train_fold, x_test_fold = mel_train_norm[train_indices], mel_train_norm[test_indices]\n",
    "    y_train_fold, y_test_fold = label_train_encoded[train_indices], label_train_encoded[test_indices]\n",
    "\n",
    "    # Create a new model for each fold based on your choice\n",
    "    if run_vanilla_vit:\n",
    "        model = create_vit_classifier(vanilla=True)\n",
    "    else:\n",
    "        model = create_vit_classifier(vanilla=False)\n",
    "\n",
    "    # Run the experiment for this fold\n",
    "    history = run_experiment(model, x_train_fold, y_train_fold, x_test_fold, y_test_fold)\n",
    "\n",
    "    # Append results to lists\n",
    "    all_train_histories.append(history)\n",
    "    all_test_accuracies.append(history.history['val_accuracy'][-1])\n",
    "    all_test_top_5_accuracies.append(history.history['val_top-5-accuracy'][-1])\n",
    "\n",
    "# Calculate and print the average performance across all folds\n",
    "avg_test_accuracy = sum(all_test_accuracies) / num_folds\n",
    "avg_test_top_5_accuracy = sum(all_test_top_5_accuracies) / num_folds\n",
    "\n",
    "print(f\"Average test accuracy across all folds: {round(avg_test_accuracy * 100, 2)}%\")\n",
    "print(f\"Average test top 5 accuracy across all folds: {round(avg_test_top_5_accuracy * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
