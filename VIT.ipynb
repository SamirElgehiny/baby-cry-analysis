{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import math\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Setting seed for reproducibiltiy\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_path(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        images.append(tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(os.path.join(path, file), target_size=(32, 32, 3))))\n",
    "        labels.append((label))\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_train = []\n",
    "mel_test = []\n",
    "label_train = []\n",
    "label_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/train/pain', 0)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/train/burping', 1)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/train/discomfort', 2)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/train/hungry', 3)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/train/tired', 4)\n",
    "    \n",
    "mel_train += images\n",
    "label_train += labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/test/pain', 0)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/test/burping', 1)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/test/discomfort', 2)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/test/hungry', 3)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('/Users//Desktop/Datasets/spectro_aug_tt/test/tired', 4)\n",
    "    \n",
    "mel_test += images\n",
    "label_test += labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_augment(spec, num_time_drop=2, num_freq_drop=2, max_time_stripes=2, max_freq_stripes=2):\n",
    "    augmented_spec = spec.copy()\n",
    "\n",
    "    # Apply time dropping\n",
    "    for _ in range(num_time_drop):\n",
    "        time_start = np.random.randint(0, spec.shape[0] - 1)\n",
    "        time_end = np.random.randint(time_start + 1, spec.shape[0])\n",
    "        augmented_spec[time_start:time_end, :] = 0\n",
    "\n",
    "    # Apply frequency dropping\n",
    "    for _ in range(num_freq_drop):\n",
    "        freq_start = np.random.randint(0, spec.shape[1] - 1)\n",
    "        freq_end = np.random.randint(freq_start + 1, spec.shape[1])\n",
    "        augmented_spec[:, freq_start:freq_end] = 0\n",
    "\n",
    "    # Apply time warping\n",
    "    for _ in range(max_time_stripes):\n",
    "        time_start = np.random.randint(0, spec.shape[0] - 1)\n",
    "        time_end = np.random.randint(time_start + 1, spec.shape[0])\n",
    "        time_mid = np.random.randint(time_start, time_end)\n",
    "        time_mid_val = spec[time_mid, :]\n",
    "        augmented_spec[time_start:time_mid, :] = augmented_spec[time_start:time_mid, :][::-1]\n",
    "        augmented_spec[time_mid:time_end, :] = augmented_spec[time_mid:time_end, :][::-1]\n",
    "        augmented_spec[time_start:time_end, :] = time_mid_val\n",
    "\n",
    "    # Apply frequency warping\n",
    "    for _ in range(max_freq_stripes):\n",
    "        freq_start = np.random.randint(0, spec.shape[1] - 1)\n",
    "        freq_end = np.random.randint(freq_start + 1, spec.shape[1])\n",
    "        freq_mid = np.random.randint(freq_start, freq_end)\n",
    "        freq_mid_val = spec[:, freq_mid]\n",
    "        freq_mid_val = np.expand_dims(freq_mid_val, axis=1)\n",
    "        freq_mid_val = np.repeat(freq_mid_val, freq_end - freq_start, axis=1)\n",
    "        augmented_spec[:, freq_start:freq_end] = freq_mid_val\n",
    "\n",
    "    augmented_spec = resize(augmented_spec, (32, 32, 3), preserve_range=True)\n",
    "\n",
    "    return augmented_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mel_train_norm = np.array(mel_train) / 255\n",
    "mel_test_norm = np.array(mel_test) / 255\n",
    "\n",
    "\n",
    "label_train_encoded = to_categorical(label_train)\n",
    "label_test_encoded = to_categorical(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the count of each class in the training set\n",
    "class_counts = np.sum(label_train_encoded, axis=0)\n",
    "\n",
    "# Find the majority class count\n",
    "majority_class_count = np.max(class_counts)\n",
    "\n",
    "# Augment the minority classes to match the majority class count using SpecAugment\n",
    "for i, count in enumerate(class_counts):\n",
    "    if count < majority_class_count:\n",
    "        # Determine the number of SpecAugment augmentations required for the current class\n",
    "        num_augmentations = int(majority_class_count - count)\n",
    "\n",
    "        # Get the indices of the samples belonging to the current class\n",
    "        class_indices = np.where(label_train_encoded[:, i] == 1)[0]\n",
    "\n",
    "        # Randomly select samples from the current class for SpecAugment augmentation\n",
    "        selected_indices = np.random.choice(class_indices, size=num_augmentations, replace=True)\n",
    "\n",
    "        # Apply SpecAugment augmentation to the selected samples\n",
    "        augmented_mel_train = []\n",
    "        augmented_labels = []\n",
    "        for idx in selected_indices:\n",
    "            augmented_mel_spec = spec_augment(mel_train_norm[idx])\n",
    "            augmented_label = label_train_encoded[idx]\n",
    "            augmented_mel_train.append(augmented_mel_spec)\n",
    "            augmented_labels.append(augmented_label)\n",
    "        augmented_mel_train = np.array(augmented_mel_train)\n",
    "        augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "        # Concatenate the augmented samples with the original training data and labels\n",
    "        mel_train_norm = np.concatenate((mel_train_norm, augmented_mel_train))\n",
    "        label_train_encoded = np.concatenate((label_train_encoded, augmented_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1525, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1525, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "BUFFER_SIZE = 512\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 72\n",
    "PATCH_SIZE = 6\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 220\n",
    "\n",
    "# ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "TRANSFORMER_LAYERS = 8\n",
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "MLP_HEAD_UNITS = [2048, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(mel_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedPatchTokenization(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        projection_dim=PROJECTION_DIM,\n",
    "        vanilla=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.half_patch = patch_size // 2\n",
    "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "\n",
    "    def crop_shift_pad(self, images, mode):\n",
    "        # Build the diagonally shifted images\n",
    "        if mode == \"left-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = 0\n",
    "            shift_width = 0\n",
    "        elif mode == \"left-down\":\n",
    "            crop_height = 0\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = 0\n",
    "        elif mode == \"right-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = 0\n",
    "            shift_height = 0\n",
    "            shift_width = self.half_patch\n",
    "        else:\n",
    "            crop_height = 0\n",
    "            crop_width = 0\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = self.half_patch\n",
    "\n",
    "        # Crop the shifted images and pad them\n",
    "        crop = tf.image.crop_to_bounding_box(\n",
    "            images,\n",
    "            offset_height=crop_height,\n",
    "            offset_width=crop_width,\n",
    "            target_height=self.image_size - self.half_patch,\n",
    "            target_width=self.image_size - self.half_patch,\n",
    "        )\n",
    "        shift_pad = tf.image.pad_to_bounding_box(\n",
    "            crop,\n",
    "            offset_height=shift_height,\n",
    "            offset_width=shift_width,\n",
    "            target_height=self.image_size,\n",
    "            target_width=self.image_size,\n",
    "        )\n",
    "        return shift_pad\n",
    "\n",
    "    def call(self, images):\n",
    "        if not self.vanilla:\n",
    "            # Concat the shifted images with the original image\n",
    "            images = tf.concat(\n",
    "                [\n",
    "                    images,\n",
    "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "        # Patchify the images and flatten it\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        flat_patches = self.flatten_patches(patches)\n",
    "        if not self.vanilla:\n",
    "            # Layer normalize the flat patches and linearly project it\n",
    "            tokens = self.layer_norm(flat_patches)\n",
    "            tokens = self.projection(tokens)\n",
    "        else:\n",
    "            # Linearly project the flat patches\n",
    "            tokens = self.projection(flat_patches)\n",
    "        return (tokens, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAFICAYAAADd1gwNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAGjklEQVR4nO3dwY7iSAJFUWzV/39y0KtaYLluGxJHueCcHRqUr9UaXcV4sL3c7/f7DYBd69/+BwC4MpEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAoRfR7+4ruf0dIzx8HlZllN2tjcWzdqZuWXn2jszt+y8trPHSRIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQDh8WyLAv+DdtzA6SQIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECMv9yOvCAL6UkyRAEEmAIJIA4fCj0tb1nJ6OMR4+v/sxR79tL73O2pm5ZefaOzO3vnnnme9u+7PHSRIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAHCct97pRsAt9vNSRIgiSRAEEmA8OvoF9f1nJ6OMR4+L8tyys720uusnZlbdq69M3Prm3ee+e62P3ucJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgChMO3JW55whrwDZwkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAKE5e7+QoA/cpIECCIJEEQSIBx+VNq6Pvb0XZcyt39nWZa3/N2r7MzcsnPtnZlb37zzzHfHGP/7HSdJgCCSAEEkAcLLr28461oEwJU4SQIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAoTl/q7XHgJ8ICdJgCCSAEEkAcLhtyWu6zk9HWM8fD7rLYzbS6+zdmZu2bn2zsytb9555rvb/uxxkgQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYCw3Pde6QbA7XZzkgRIIgkQRBIg/Dr6xXU9p6djjIfPy7KcsrO99DprZ+aWnWvvzNyys2/7d7b92eMkCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEA4/mRzgX/Dup6U7SQIEkQQIIgkQlvveK90AuN1uTpIASSQBgkgChMO/k1zXc3o6xnj4/O7fOP22vfQ6a2fmlp1r78zc+uadZ7677c8eJ0mAIJIAQSQBwsv3bvt5JXBF1aZXrqE6SQIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAoTl7hHjAH/kJAkQRBIgiCRAOPy2xHU9p6djjIfPr7zN7IjtpddZOzO37Fx7Z+bWN+88891tf/Y4SQIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgHD4UWkA/4J6I80rj3ZzkgQIIgkQRBIgiCRAEEmAIJIAYbnX/18O8OWcJAGCSAIEkQQIh29LXNdzejrGePj8ym1DR2wvvc7ambll59o7M7fs7Nv+nW1/9jhJAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQI3pYIfJR33yrpJAkQRBIgiCRAEEmAIJIAQSQBgp8AAR+lXgD7ys+DnCQBgkgCBJEECMu9/gc8wJdzkgQIIgkQRBIgHP6d5Lqe09MxxsPndz/m6LftpddZOzO37Fx7Z+aWnX3bv7Ptzx4nSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAwesbgI/y7lslnSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAoTlvvdKNwBut5uTJEASSYAgkgDh8KPS1vWcno4xHj6/+zFHv20vvc7ambll59o7M7e+eeeZ7277s8dJEiCIJEAQSYAgkgBBJAGCSAIEb0sEPsq777R2kgQIIgkQRBIgiCRAEEmAIJIAQSQBwuV+J3nWo5f+1s7MrZ/snPUWj5/8M/0L/96uuvVpO3+TkyRAEEmAIJIAQSQBgkgCBJEECMv9rN9+AHwAJ0mAIJIAQSQBwuHbEtf1nJ6OMT56Z+bWT3bq0vT2P3vmVrRnvuu/C+/bsvPazh4nSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIA4XJvS+TvOOtWw588ie8nt0PCuzhJAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIy/0n940BfDgnSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmA8B8P0QfCNZQWoAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEFT-UP\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEFT-DOWN\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT-UP\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT-DOWN\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a random image from the training dataset\n",
    "# and resize the image\n",
    "image = mel_train_norm[np.random.choice(range(mel_train_norm.shape[0]))]\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    ")\n",
    "\n",
    "# Vanilla patch maker: This takes an image and divides into\n",
    "# patches as in the original ViT paper\n",
    "(token, patch) = ShiftedPatchTokenization(vanilla=True)(resized_image / 255.0)\n",
    "(token, patch) = (token[0], patch[0])\n",
    "n = patch.shape[0]\n",
    "count = 1\n",
    "plt.figure(figsize=(4, 4))\n",
    "for row in range(n):\n",
    "    for col in range(n):\n",
    "        plt.subplot(n, n, count)\n",
    "        count = count + 1\n",
    "        image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 3))\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Shifted Patch Tokenization: This layer takes the image, shifts it\n",
    "# diagonally and then extracts patches from the concatinated images\n",
    "(token, patch) = ShiftedPatchTokenization(vanilla=False)(resized_image / 255.0)\n",
    "(token, patch) = (token[0], patch[0])\n",
    "n = patch.shape[0]\n",
    "shifted_images = [\"ORIGINAL\", \"LEFT-UP\", \"LEFT-DOWN\", \"RIGHT-UP\", \"RIGHT-DOWN\"]\n",
    "for index, name in enumerate(shifted_images):\n",
    "    print(name)\n",
    "    count = 1\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for row in range(n):\n",
    "        for col in range(n):\n",
    "            plt.subplot(n, n, count)\n",
    "            count = count + 1\n",
    "            image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 5 * 3))\n",
    "            plt.imshow(image[..., 3 * index : 3 * index + 3])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "\n",
    "    def call(self, encoded_patches):\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_patches = encoded_patches + encoded_positions\n",
    "        return encoded_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is\n",
    "        # the square root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Build the diagonal attention mask\n",
    "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier(vanilla=False):\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder()(tokens)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        if not vanilla:\n",
    "            attention_output = MultiHeadAttentionLSA(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1, attention_mask=diag_attn_mask)\n",
    "        else:\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(5)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, model_name):\n",
    "    total_steps = int((len(mel_train_norm) / BATCH_SIZE) * EPOCHS)\n",
    "    warmup_epoch_percentage = 0.10\n",
    "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "    scheduled_lrs = WarmUpCosine(\n",
    "        learning_rate_base=LEARNING_RATE,\n",
    "        total_steps=total_steps,\n",
    "        warmup_learning_rate=0.0,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    # early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    history = model.fit(\n",
    "        x=mel_train_norm,\n",
    "        y=label_train_encoded,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(mel_test_norm, label_test_encoded),\n",
    "      #  callbacks=[early_stop],\n",
    "    )\n",
    "\n",
    "        # Make predictions on the test data\n",
    "    y_pred = model.predict(mel_test_norm)\n",
    "\n",
    "    # Convert the predictions from one-hot encoded format to class labels\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_true_labels = np.argmax(label_test_encoded, axis=1)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "    precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "    recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "    f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "    # Calculate accuracy scores for each class\n",
    "    class_accuracy = confusion_mat.diagonal() / confusion_mat.sum(axis=1)\n",
    "\n",
    "    # Print the accuracy scores for each class\n",
    "    for i, acc in enumerate(class_accuracy):\n",
    "        print(f\"Accuracy for class {i}: {acc}\")\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(mel_test_norm, label_test_encoded, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "    # Save the model\n",
    "    model.save(f\"/Users//Desktop/Datasets/T/{model_name}.h5\")\n",
    "    print(\"Model saved successfully.\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Run experiments with the vanilla ViT\n",
    "vit = create_vit_classifier(vanilla=True)\n",
    "history = run_experiment(vit, model_name=\"vanilla_vit\")\n",
    "\n",
    "# Run experiments with the Shifted Patch Tokenization and\n",
    "# Locality Self Attention modified ViT\n",
    "vit_sl = create_vit_classifier(vanilla=False)\n",
    "history = run_experiment(vit_sl, model_name=\"modified_vit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
